\section{3 \glsentrytext{dof} localization system tests}\label{sec:planar-localization-system-tests}


\subsection{Overview}

The main results of the 3 \gls{dof} tests performed with the \gls{drl} system (performing localization only with a given initial pose) are presented in \cref{tab:localization-system-evaluation_3-dof-results,tab:localization-system-evaluation_3-dof-results-odometry-amcl}. They summarize each test by fitting a normal distribution to each evaluation metric. They were retrieved with a known initial pose and used \gls{icp} point-to-point as tracking algorithm. The Jarvis and Pioneer tests had a map built using the localization system in mapping mode and were manually corrected to achieve a resolution of 10 and 25 mm respectively (shown in \Cref{fig:localization-system-evaluation_lab-10mm} and \Cref{fig:localization-system-evaluation_drl-corrected} respectively). The Guardian tests relied on a map built from the \gls{cad} model with resolution of 2 mm. Sensor data preprocessing relied on a voxel grid of 50 mm in order to reduce the impact of sensor measurement noise and also control the level of detail of the live point clouds. The initial pose estimation subsystem tests used \gls{sift} for keypoint selection, \gls{fpfh} for keypoint description and the feature matching algorithm described in \Cref{subsec:localization-system_feature-registration} to estimate the initial position and orientation of the robot (\Cref{fig:localization-system-evaluation_ship-interior-initial-pose-estimation-sift-fpfh-ransac-gicp-1} shows the accepted initial poses when the robot was in the lower right corner of the Guardian test environment and was given as the initial pose the large red arrow).

The next sections we will provide a brief analysis of the main 3 \gls{dof} results achieved with the \gls{drl} system. We will start by explaining how laser spherical interpolation can improve localization by mitigating point cloud deformation. Later on we will analyze the point cloud preprocessing stage and why it should be carefully tuned to the sensors and ambient geometry. Next we will present the mapping results and why it is necessary to have accurate environment representations in order to achieve precise localization. Finally it will be presented a global analysis of the tests results, in which the pose estimation accuracy achieved by the \gls{drl} system will be compared with the ground truth, odometry and \gls{amcl}.


\begin{sidewaystable*}
	\caption{3 \glsentrytext{dof} \glsentrytext{drl} localization test results}
	\tabulinesep = 1.2ex
	\setlength{\tabcolsep}{0.1em}
	\centering
	\scriptsize
	\begin{tabu} to \textwidth { X[m,c] X[m,c] X[m,c] X[1.7m,c] X[m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] }
		\hline
		\multicolumn{7}{c}{Test conditions} && \multicolumn{2}{c}{Translation error (mm)} && \multicolumn{2}{c}{Rotation error (degrees)} && \multicolumn{2}{c}{Outliers percentage [0..100]} && \multicolumn{2}{c}{Global computation time (ms)} \\
		\cline{1-7} \cline{9-10} \cline{12-13} \cline{15-16} \cline{18-19}
		Platform 																& Ambient 													& Path shape 											& Path velocities 		& Map cell resolution	& Spherical linear interpolation	& Nº scans / Nº lasers 	&& Mean   & Standard deviation 	&& Mean  & Standard deviation 	&& Mean  & Standard deviation 	&& Mean   & Standard deviation \\ \hline
		\multirow{4}{0.05\textwidth}{\centering Jarvis robot} 					& \multirow{3}{0.05\textwidth}{\centering Cluttered} 		& \multirow{3}{0.05\textwidth}{\centering Rounded} 		& 5 cm/s 				& 10 mm					&	Yes								& 1-4/1 				&& 4.281  & 2.264 				&& 0.441 & 0.070 				&& 24.35 & 5.17 				&& 11.776 & 12.265	\\
																				& 													 		& 														& 5 cm/s 				& 10 mm					&	No								& 1-4/1 				&& 4.957  & 2.473 				&& 0.447 & 0.087 				&& 24.34 & 5.17 				&& 11.656 & 12.997	\\
																				&															&														& 30 cm/s				& 10 mm					&	Yes								& 1-4/1					&& 12.250 & 8.901				&& 0.535 & 0.414				&& 16.08 & 3.80					&& 21.295 &	8.352	\\ \tabucline[0.5pt on 3pt off 2pt]{2-3}
																				& \multirow{3}{0.05\textwidth}{\centering Cluttered dynamic}& \multirow{3}{0.05\textwidth}{\centering Complex} 		& 5 cm/s 				& 10 mm					&	Yes								& 1-4/1			 		&& 4.280  & 2.264 				&& 0.441 & 0.070 				&& 24.35 & 5.17 				&& 11.776 & 12.265	\\
																				&															&														& {50-30-50-10 cm/s}	& 10 mm					&	Yes								& 1-4/1					&& 6.422  &	3.992				&& 0.397 & 0.099				&& 23.84 & 5.59					&& 13.376 &	13.316	\\
																				&															&														& {50-30-50-10 cm/s}	& 10 mm					&	No								& 1-4/1					&& 8.884  &	8.740				&& 0.422 & 0.142				&& 24.14 & 6.18					&& 14.830 &	23.091	\\ \tabucline[0.5pt on 3pt off 2pt]{1-3}
		\multirow{4}{0.05\textwidth}{\centering Pioneer robot}					& \multirow{4}{0.05\textwidth}{\centering Cluttered dynamic}& 360º													& 23 cm/s				& 25 mm					&	Yes								& 1/1					&& 20.105 & 10.814				&& 5.704 & 0.662				&& 11.71 & 3.37					&& 5.046  & 2.106	\\
																				&															& SLAM 1												& 26 cm/s				& 25 mm					&	Yes								& 1/1					&& 22.434 & 12.052				&& 5.429 & 0.676				&& 4.31  & 3.84					&& 4.569  & 2.530	\\
																				&															& SLAM 2												& 19 cm/s				& 25 mm					&	Yes								& 1/1					&& 19.391 & 12.568				&& 5.518 & 0.818				&& 5.09  & 4.11					&& 4.650  & 2.180	\\
																				&															& SLAM 3												& 16 cm/s				& 25 mm					&	Yes								& 1/1					&& 17.480 & 8.895				&& 5.455 & 0.732				&& 4.15  & 4.26					&& 4.388  & 2.526	\\ \tabucline[0.5pt on 3pt off 2pt]{1-3}
		\multirow{6}{0.05\textwidth}{\centering Guardian simulator (Gazebo)} 	& \multirow{2}{0.05\textwidth}{\centering Static} 			& \multirow{6}{0.05\textwidth}{\centering Wall follower}& 5 cm/s 				& 2 mm					&	Yes								& 2/2 					&& 2.642  & 0.463 				&& 0.016 & 0.023 				&& 0.0   & 0.0  				&& 5.723  & 1.868 	\\
																				&  															&  														& 30 cm/s 				& 2 mm					&	Yes								& 2-4/2					&& 4.654  & 3.893 				&& 0.044 & 0.070 				&& 0.0   & 0.0  				&& 7.784  & 3.178 	\\ \tabucline[0.5pt on 3pt off 2pt]{2-2}
																				& \multirow{2}{0.05\textwidth}{\centering Cluttered} 		&  														& 5 cm/s 				& 2 mm					&	Yes								& 2/2					&& 2.912  & 0.856 				&& 0.020 & 0.026 				&& 25.05 & 13.30 				&& 11.889 & 9.084 	\\
																				&  															& 														& 30 cm/s 				& 2 mm					&	Yes								& 2-4/2					&& 6.092  & 3.834 				&& 0.096 & 0.085 				&& 25.65 & 11.02 				&& 23.925 & 27.959 	\\ \tabucline[0.5pt on 3pt off 2pt]{2-2}
																				& \multirow{2}{0.05\textwidth}{\centering Cluttered dynamic}&  														& 5 cm/s 				& 2 mm					&	Yes								& 2/2					&& 4.593  & 3.802 				&& 0.069 & 0.072 				&& 33.33 & 9.88  				&& 22.217 & 32.408 	\\
																				&  															&  														& 30 cm/s 				& 2 mm					&	Yes								& 2/2					&& 5.897  & 4.452 				&& 0.095 & 0.104 				&& 27.28 & 13.04 				&& 24.409 & 33.459 	\\
		\hline
	\end{tabu}
	\label{tab:localization-system-evaluation_3-dof-results}
	
	\bigskip
	
	\caption{3 \glsentrytext{dof} odometry and \glsentrytext{amcl} localization test results}
	\begin{tabu} to \textwidth { X[m,c] X[m,c] X[m,c] X[1.7m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] }
		\hline
		\multicolumn{5}{c}{Test conditions} && \multicolumn{2}{c}{Odometry translation error (mm)} && \multicolumn{2}{c}{Odometry rotation error (degrees)} && \multicolumn{2}{c}{\glsentrytext{amcl} translation error (mm)} && \multicolumn{2}{c}{\glsentrytext{amcl} rotation error (degrees)} \\
		\cline{1-5} \cline{7-8} \cline{10-11} \cline{13-14} \cline{16-17}
		Platform 																& Ambient 													& Path shape 											& Path velocities 		& Map cell resolution 	&& Mean   	& Standard deviation 	&& Mean  	& Standard deviation 	&& Mean  	& Standard deviation 	&& Mean   & Standard deviation \\ \hline
		\multirow{4}{0.05\textwidth}{\centering Jarvis robot} 					& \multirow{2}{0.05\textwidth}{\centering Cluttered} 		& \multirow{2}{0.05\textwidth}{\centering Rounded} 		& 5 cm/s 				& 10 mm					&& 25.353 	& 10.075 				&& 0.352 	& 0.206 				&& 51.417	& 20.829 				&& 0.442  & 0.174	\\
																				&															&														& 30 cm/s				& 10 mm					&& 103.676	& 50.992				&& 2.173 	& 1.916					&& 81.368	& 36.548				&& 1.388  &	1.085	\\ \tabucline[0.5pt on 3pt off 2pt]{2-3}
																				& \multirow{2}{0.05\textwidth}{\centering Cluttered dynamic}& \multirow{2}{0.05\textwidth}{\centering Complex} 		& 5 cm/s 				& 10 mm					&& 21.506 	& 11.937 				&& 0.535 	& 0.360 				&& 64.300	& 13.750 				&& 0.316  & 0.214	\\
																				&															&														& 50-30-50-10 cm/s		& 10 mm					&& 124.269	& 79.775				&& 2.723 	& 1.496					&& 84.230	& 29.134				&& 0.595  &	0.581	\\ \tabucline[0.5pt on 3pt off 2pt]{1-3}
		\multirow{4}{0.05\textwidth}{\centering Pioneer robot}					& \multirow{4}{0.05\textwidth}{\centering Cluttered dynamic}& 360º													& 23 cm/s				& 25 mm					&& 482.439	& 163.657				&& 12.810	& 4.468					&& 135.442	& 66.445				&& 6.462  & 1.290	\\
																				&															& SLAM 1												& 26 cm/s				& 25 mm					&& 370.283 	& 163.734				&& 8.351 	& 2.825					&& 111.470 	& 46.783				&& 6.496  & 1.386	\\
																				&															& SLAM 2												& 19 cm/s				& 25 mm					&& 686.983 	& 361.666				&& 11.333 	& 5.423					&& 95.537  	& 47.560				&& 6.957  & 2.294	\\
																				&															& SLAM 3												& 16 cm/s				& 25 mm					&& 295.714 	& 151.793				&& 8.858 	& 3.331					&& 108.690 	& 52.341				&& 6.463  & 1.363	\\
		\hline
	\end{tabu}
	\label{tab:localization-system-evaluation_3-dof-results-odometry-amcl}
\end{sidewaystable*}


\subsection{Laser assembly with spherical linear interpolation}

The localization system was designed to operate with any kind of point cloud sensors. For the particular case of data retrieved using \glspl{lidar}, there is a very significant problem of point cloud deformation when the robot is moving or rotating at high speeds. This is due to the fact that a typical \gls{lidar} outputs laser scans at a very low rate (8-10 Hz), and as such, assuming that the robot isn't moving when capturing an entire scan will result in deformed point clouds.

Given that \gls{ros} outputs laser scans (an entire slice of laser measurements from the start to the end of its field of view), and the localization system doesn't have access to individual measurements as they arrive, then one way to mitigate this scan deformation is by using odometry and / or \gls{imu} information to update the robot pose between the pose corrections performed by the localization system. This allows to use spherical linear interpolation when converting from the raw measurements in polar coordinates into the required Cartesian coordinates in the map frame.

The laser scan deformation is negligible when the robot is moving very slowly. However, when traveling at higher speeds, the point cloud deformation poses a serious issue for map matching algorithms, and the usage of spherical linear interpolation is very important to mitigate it.

\Cref{fig:localization-system-evaluation_laser-deformation-1,fig:localization-system-evaluation_laser-deformation-2} were retrieved using the Jarvis robot with velocities of 50-30-50-10 cm/s and without using the spherical linear interpolation module. \Cref{fig:localization-system-evaluation_laser-deformation-1-corrected,fig:localization-system-evaluation_laser-deformation-2} were taken from the same test but now using the spherical linear interpolation module. In \Cref{fig:localization-system-evaluation_laser-deformation-1} can be seen that the laser measurements in front of the robot were deformed outwards, causing the points to be projected outside both walls. \Cref{fig:localization-system-evaluation_laser-deformation-2} shows a similar effect, which is now very noticeable since the first laser measurement isn't close to the last one. These deformations were severely diminished when the laser spherical linear interpolation module was used (as can be seen in \cref{fig:localization-system-evaluation_laser-deformation-1-corrected,fig:localization-system-evaluation_laser-deformation-2-corrected} respectively.

Analyzing the tests in \cref{tab:localization-system-evaluation_3-dof-results} associated to the \crefrange{fig:localization-system-evaluation_laser-deformation-1}{fig:localization-system-evaluation_laser-deformation-2-corrected} it can be seen that using the spherical linear interpolation module allowed to reduce the mean (from 8.88 mm to 6.42 mm) and standard deviation (from 8.74 mm to 3.99 mm) of the translation error and also reduced the mean (from 0.42º to 0.39º) and standard deviation (from 0.14º to 0.10º) of the rotation error. Moreover, given that the correction of deformation gives a point cloud more similar to the map, the usage of the spherical linear interpolation module also allowed to reduced the global computation time (mean from 14.83 ms to 13.38 ms and standard deviation from 23.09 ms to 13.32 ms).

The laser assembly parameters present in \cref{tab:localization-system-evaluation_3-dof-results,tab:localization-system-evaluation_3-dof-results-odometry-amcl} as "Nº scans / Nº lasers" have the meaning of how many laser scans were being merged into each published \emph{sensor\_msgs::PointCloud2} and from how many sensors those lasers scans were being generated. As such, a configuration of "2-4/2" means that the laser assembler was merging between 2 (moving fast) and 4 (moving slow) lasers scans depending on the velocity of the robot, and these laser scans were being generated by two \glspl{lidar} sensors (one in the front and another in the back of the robot).


\begin{figure}[H]
	\centering
	\includegraphics[width=0.49\textwidth]{localization-system-evaluation/tests-3dof/laser-spherical-interpolation/laser-deformation-1}
	\caption{Large laser deformation on opposite walls when the robot is rotating (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_laser-deformation-1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.49\textwidth]{localization-system-evaluation/tests-3dof/laser-spherical-interpolation/laser-deformation-1-corrected}
	\caption{Correction of the large laser deformation on opposite walls with spherical linear interpolation  (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_laser-deformation-1-corrected}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.49\textwidth]{localization-system-evaluation/tests-3dof/laser-spherical-interpolation/laser-deformation-2}
	\caption{Laser deformation when the robot is rotating  (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_laser-deformation-2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.49\textwidth]{localization-system-evaluation/tests-3dof/laser-spherical-interpolation/laser-deformation-2-corrected}
	\caption{Correction of the laser deformation with spherical linear interpolation  (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_laser-deformation-2-corrected}
\end{figure}



\subsection{Point cloud preprocessing}

The statistical error of the lasers can be mitigated by merging several laser scans before performing a point cloud registration. This is based on the fact that a considerable amount of laser noise results in measurements oscillating around the true distance to the obstacles, and as such, the effective measurement error can be significantly reduced by retrieving the centroids of a voxel grid applied over the laser data. However, assembling too much laser scans can lead to worse pose tracking if the robot is moving at high speeds, because the lasers would be projected into the map frame with a high position error (because odometry accuracy decreases when the robot speed increases and the localization system will not correct it). As such, the implemented laser assembler supports dynamic reconfiguration of the number of laser scans to assemble (or the period of assembly time) based on the robot estimated velocity. This feature besides improving the pose tracking, it also allows a navigation supervisor to regulate the rate at which the localization system operates. This can be very useful for mobile robot manipulators because the localization system can have a high update rate when the robot is moving fast and a low update rate when it is moving slower or when it is stopped. This allows a more efficient usage of the platform hardware resources while also improving the localization system accuracy.

Besides reducing the sensor measurement errors, this processing stage also allows the removal of laser shadow points caused by veiling, or small cluster of points that may be associated with temporary objects / people. However these filters can take some time to compute given the intensive usage of point neighbors searches. For typical usage scenarios of a mobile robot manipulator, the voxel grid and random sampling filters are usually enough to control the computational resources that will be required by the registration algorithms while also improving robustness against sensor noise.



\subsection{Point cloud registration}

Looking at the results from \cref{tab:localization-system-evaluation_3-dof-results,tab:localization-system-evaluation_3-dof-results-odometry-amcl}, the poses from \Cref{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-paths}, the laser assembly figures shown in \cref{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-drl-cumulative,fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-amcl-cumulative} and the translation / rotation error graphs presented from \crefrange{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-translation-error-amcl}{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-rotation-error} (from the Jarvis test in the complex path at high velocities), it can be seen that the localization system can register point clouds with much more accuracy than the \gls{amcl} \gls{ros} package (\gls{amcl} was using at most 5000 particles). For the test shown in \cref{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-paths}, the \gls{drl} system was 13.12 times more accurate than the \gls{amcl} package in computing the robot position (achieved 6.422 mm of mean translation error while \gls{amcl} had 84.23 mm) and was also 1.5 times more accurate than the \gls{amcl} package in computing the robot orientation (achieved 0.397º of mean rotation error while \gls{amcl} had 0.595º).

The most common problems that seemed to affect the registration algorithms of the \gls{drl} system were the point cloud deformation (usually when there is unreliable odometry information, which typically occurs when the robot is moving with high velocities / accelerations), laser measurements errors, unknown objects close to the known reference point cloud and also low resolution maps. The localization system can tolerate these problems by having a default pipeline configuration for the normal operation of the robot, another for temporary tracking recovery and yet another for initial pose estimation.

The switch between the localization system operation modes using the point cloud registration analysis proved to be a very intuitive and fast process to setup and the overall system configurations seemed to be generic enough to be reused in several types of environments, with different robots equipped with varying types of sensors. Such ease of configuration allows the fast deployment of robots and gives a high confidence that the localization system will remain accurate even in challenging environments. This is not the case with systems such as \gls{amcl}, given that they are very dependent on the odometry and laser models, and as such, require constant retuning when one of these models change.

The ability to switch registration algorithms at runtime based on the quality of the estimated pose proved to be an efficient and flexible architecture choice, since it allowed high precision pose tracking with fast registration algorithms and occasional pose tracking recovery with more robust methods / configurations (which happened more often in the tests at higher velocities, in which the odometry was significantly worse).

The initial pose estimation using feature matching was very reliable and successfully found the robot location even in low feature surroundings (as can be seen in \cref{fig:localization-system-evaluation_jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-1,fig:localization-system-evaluation_ship-interior-initial-pose-estimation-sift-fpfh-ransac-gicp-1}). Moreover, the output of the accepted initial pose estimations allows the detection of similar map locations by a navigation supervisor, which can then plot a path to disambiguate the initial pose guess and avoid dangerous operations at a wrong position.

\Crefrange{fig:localization-system-evaluation_jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-1}{fig:localization-system-evaluation_ship-interior-initial-pose-estimation-sift-fpfh-ransac-gicp-2} show the accepted initial pose estimations using the global localization subsystem presented in \cref{subsec:localization-system_feature-registration}.

The blue circles presented in \Cref{fig:localization-system-evaluation_jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-1} represent the keypoints of the live point cloud in the robot start up position, while the violet circles are the reference point cloud keypoints. The green circles are the live point cloud laser measurements after performing the initial pose estimation and registration refinement.


\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.45\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/graphs/robot-movement-path-with-odometry-and-amcl}
	\caption{Poses estimated by the ground truth, localization system, \glsentrytext{amcl} and odometry}
	\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-paths}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/drl-cumulative}
	\caption{Laser scans assembled on top of the map using the localization system poses (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-drl-cumulative}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/amcl-cumulative}
	\caption{Laser scans assembled on top of the map using the \glsentrytext{amcl} poses (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-amcl-cumulative}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/initial_pose_estimation/jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-1}
	\caption{Overview of initial pose estimation using feature matching in Jarvis environment (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/initial_pose_estimation/jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-3}
	\caption{Initial pose estimation using feature matching in Guardian static environment (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_jarvis-initial-pose-estimation-sift-fpfh-ransac-gicp-2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/initial_pose_estimation/guardian-initial-pose-estimation-sift-fpfh-ransac-gicp-1}
	\caption{Overview of initial pose estimation using feature matching in Guardian static environment (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_ship-interior-initial-pose-estimation-sift-fpfh-ransac-gicp-1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/initial_pose_estimation/guardian-initial-pose-estimation-sift-fpfh-ransac-gicp-2}
	\caption{Initial pose estimation using feature matching in Guardian static environment (grid with 1 m spacing)}
	\label{fig:localization-system-evaluation_ship-interior-initial-pose-estimation-sift-fpfh-ransac-gicp-2}
\end{figure}




\subsection{Translation and rotation errors}

By analyzing \cref{tab:localization-system-evaluation_3-dof-results,tab:localization-system-evaluation_3-dof-results-odometry-amcl} it can be seen that the proposed 3 \gls{dof} localization system can achieve pose tracking with less than 1 cm in translation error and less than 1 degree in rotation error when using a detailed map (10 mm cell resolution) and a sensor with good field of view (360º) and low sensor noise (15 mm). When using a sensor with low field of view (180º) and high sensor noise (35 mm), the \gls{drl} system still managed to achieve localization accuracy bellow the map resolution (translation error less than 2 cm and rotation error close to 5 degrees).

As expected, the tests at lower velocities had less mean error than the ones at higher speeds while requiring slightly less computation time. Also, the simulator tests had less mean error than the ones performed on the physical platforms. This is due to laser scan deformation that couldn't be simulated and also because the Gazebo simulator was only able to add Gaussian noise to the laser measurements. To compensate these limitations, the error in odometry and laser measurements in simulation was deliberately higher than the expected values for the physical platforms. This explains why the localization system needed more time to perform the pose estimations in the simulator tests. It can also be seen that adding unknown objects increased the mean error and required computation time while adding dynamic objects increased these values even further (given that a moving object appears in a laser scan as a deformed version of its static shape).

Comparing the results of the tests performed with Jarvis robot  with the tests of the Pioneer robot, it can be seen that even when severely reducing the laser range (from 80 to 6 meters), moving the robot in dynamic environments with large occluding objects and even when using a low resolution map (25 mm in the Pioneer tests vs 10 mm in the Jarvis tests), the localization system still managed to track the robot pose with mean translation error (17 mm) below the map resolution (25 mm).

These results show that the localization system can reliably achieve high accuracy pose tracking even in dynamic and challenging environments.

\begin{figure}[H]
	\centering
	\begin{minipage}[H]{0.49\textwidth}
		\centering
		\includegraphics[trim={0 0 0 1cm}, clip, width=0.97\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/graphs/translation-error-millimeters-distributions-amcl}
		\caption{Probability distributions for the \glsentrytext{amcl} translation errors}
		\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-translation-error-amcl}
	\end{minipage}\hfill
	\begin{minipage}[H]{0.49\textwidth}
		\centering
		\includegraphics[trim={0 0 0 1cm}, clip, width=0.97\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/graphs/rotation-error-degrees-distributions-amcl}
		\caption{Probability distributions for the \glsentrytext{amcl} rotation errors}
		\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-rotation-error-amcl}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}[H]{0.49\textwidth}
		\centering
		\includegraphics[trim={0 0 0 1cm}, clip, width=0.97\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/graphs/translation-error-millimeters-distributions}
		\caption{Probability distributions for the localization system translation errors}
		\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-translation-error}
	\end{minipage}\hfill
	\begin{minipage}[H]{0.49\textwidth}
		\centering
		\includegraphics[trim={0 0 0 1cm}, clip, width=0.97\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/graphs/rotation-error-degrees-distributions}
		\caption{Probability distributions for the localization system rotation errors}
		\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-rotation-error}
	\end{minipage}
\end{figure}



\subsection{Computation time}

Looking at the computation time graphs in \Cref{tab:localization-system-evaluation_3-dof-results} and to \Cref{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-computation-time} it can be seen that the localization system can register the point clouds very fast (between 5 and 30 milliseconds), which is low enough to process all the incoming laser scans in real time (typically \glspl{lidar} generate scans every 100 milliseconds). Moreover, the computation time seems to be very stable, with occasional peaks due to laser deformation or varying percentage of outliers.

The global computation time is mostly associated with the point cloud registration stage, while the rest of it is due to normal estimation and point cloud preprocessing algorithms.

\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.49\textwidth]{localization-system-evaluation/tests-3dof/jarvis-robot/complex-outliers-50-30-50-10-1-4-scans/graphs/computation-times-milliseconds-global-time-distributions}
	\caption{Probability distributions for the localization system global computation time}
	\label{fig:localization-system-evaluation_complex-path-with-outliers-50-30-50-10cm-per-sec-velocity-1-4-scans-computation-time}
\end{figure}


\subsection{Mapping}

Any self-localization system requires a map to estimate the pose of the robot when using exteroceptive information. If such map doesn't exist, then the first live point cloud can be considered as the initial map, and then it can be updated dynamically as new sensor data is processed.

The dynamic and incremental map update capability of the \gls{drl} system can be used to register the full sensor point clouds or only the inliers / outliers. Full integration is useful when starting a map from scratch (example in \Cref{fig:localization-system-evaluation_drl-2.5cm-1.0-bag-speed}). Partial integration can be useful when there is a highly detailed map (for example generated from a \gls{cad} model or with other high accuracy mapping system) and we only want to add or remove information from it, such as integrating new large objects and opening doors in the map (examples from \crefrange{fig:localization-system-evaluation_indoor-10mm}{fig:localization-system-evaluation_indoor-10mm-mapping-partial-integration}). Partial integration besides reducing the computational resources required it also allows to keep the detail of the original map (by avoiding deformations due to sensor measurement noise). This can be clearly seen in \Cref{fig:localization-system-evaluation_lab-10mm-dynamic} in which the sensor noise polluted the walls of the original map (given that the map cell resolution was much smaller than the mean sensor noise). By performing selective mapping this problem was avoided in the map present in \Cref{fig:localization-system-evaluation_indoor-10mm-dynamic} (by inserting on the map only unknown zones while keeping the walls generated from the \gls{cad} model untouched).

Looking at planar structures from \crefrange{fig:localization-system-evaluation_drl-2.5cm-1.0-bag-speed}{fig:localization-system-evaluation_gmapping-2.5cm-1.0-bag-speed} it can be seen that the \gls{drl} system was able to achieve equal or even better mapping results than the GMapping\footnote{\url{http://wiki.ros.org/gmapping}} \gls{slam} system and even the ground truth provided by the Raptor-E cameras (that seems to be less suitable for mapping than both the \gls{drl} system and the GMapping \gls{ros} package).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.43\textwidth]{localization-system-evaluation/tests-3dof/maps/industrial-hall/drl-corrected}
	\caption{Map manually corrected with 25 mm cell resolution (based on \Cref{fig:localization-system-evaluation_drl-2.5cm-1.0-bag-speed}) }
	\label{fig:localization-system-evaluation_drl-corrected}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/industrial-hall/drl-2.5cm-1.0-bag-speed}
	\caption{Map made with the localization system using full integration in conjunction with OctoMap}
	\label{fig:localization-system-evaluation_drl-2.5cm-1.0-bag-speed}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/industrial-hall/ground-truth-2.5cm-0.245-time-offset}
	\caption{Map made using the ground truth poses provided by the Raptor-E cameras}
	\label{fig:localization-system-evaluation_ground-truth-2.5cm-0.245-time-offset}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/industrial-hall/gmapping-2.5cm-0.1-bag-speed}
	\caption{Map made with the GMapping package playing the rosbag at 10\% speed}
	\label{fig:localization-system-evaluation_gmapping-2.5cm-0.1-bag-speed}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/industrial-hall/gmapping-2.5cm-1.0-bag-speed}
	\caption{Map made with the GMapping package playing the rosbag in real time}
	\label{fig:localization-system-evaluation_gmapping-2.5cm-1.0-bag-speed}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/indoor-10mm}
	\caption{Map of structured environment generated using a CAD model}
	\label{fig:localization-system-evaluation_indoor-10mm}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/indoor-10mm-mapping-partial-integration}
	\caption{Updated map of structured environment using partial integration (only points that were not close to the walls were integrated)}
	\label{fig:localization-system-evaluation_indoor-10mm-mapping-partial-integration}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/indoor-10mm-mapping-full-integration}
	\caption{Updated map of structured environment using full integration (all registered were integrated)}
	\label{fig:localization-system-evaluation_indoor-10mm-mapping-full-integration}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/indoor-10mm-mapping}
	\caption{Mapping of the structured environment using full integration (all registered were integrated)}
	\label{fig:localization-system-evaluation_indoor-10mm-mapping}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/lab-10mm}
	\caption{RoboCup field map (manually corrected)}
	\label{fig:localization-system-evaluation_lab-10mm}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-3dof/maps/lab-10mm-dynamic}
	\caption{Updated map of RoboCup field using full integration (starting with the map from \Cref{fig:localization-system-evaluation_lab-10mm})}
	\label{fig:localization-system-evaluation_lab-10mm-dynamic}
\end{figure}



%\clearpage

\section{6 \glsentrytext{dof} localization system tests}\label{sec:tridimensional-localization-system-tests}

\subsection{Overview}

The main 6 \gls{dof} results retrieved with the localization system are shown in \Cref{tab:localization-system-evaluation_6-dof-results}. The first two experiments (fly and translations movements) were performed to evaluate the accuracy of the point cloud registration algorithms while the last one (mainly rotation movements) aimed to test the robustness against temporary absence of valid sensor data (in this test the Kinect had periods in which most of its field of view was outside the map).

These tests were retrieved with a known initial pose and used \gls{icp} point-to-point as tracking algorithm and \gls{icp} point-to-plane as tracking recovery method. The tests with the ethzasl\_icp\_mapper used the \gls{icp} point-to-plane since the implementation of the \gls{icp} point-to-point was consistently losing tracking (the authors of this systems also point out in \cite{Pomerleau2013a} that \gls{icp} point-to-plane implementation was more robust and achieved more accurate tracking).

The 3D map was done with the localization system in \gls{slam} mode and used continuous surface reconstruction in order to create an accurate representation of the environment and reduce the impact of the measurements noise. This map was later downsampled using a voxel grid with 20 mm cells in order to allow real-time processing of the Kinect data.

The next sections will provide an analysis of the 6 \gls{dof} results achieved with the \gls{drl} system. It will start by explaining the importance of point cloud processing and how it helps reduce the cloud registration time. Then it will give an overview of the 3D map building capabilities and it will finish with an analysis of the translation and rotation errors along with the computation time.


\begin{sidewaystable}
	\caption{6 \glsentrytext{dof} results}
	\tabulinesep = 1.2ex
	\setlength{\tabcolsep}{0.2em}
	\centering
	\scriptsize
	\begin{tabu} to \textwidth { X[m,c] X[1.7m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] X[m,c] X[0.01m,c] X[m,c] }
		\hline
		\multicolumn{4}{c}{Test conditions} 												&& \multicolumn{2}{c}{Translation error (mm)} && \multicolumn{2}{c}{Rotation error (degrees)} && \multicolumn{2}{c}{Outliers percentage [0..100]} && \multicolumn{2}{c}{Global computation time (ms)} && Registrations \\
		\cline{1-4} \cline{6-7} \cline{9-10} \cline{12-13} \cline{15-16} \cline{18-18}
		Path shape 														& Path velocity 	& Map cell resolution 	& System && Mean   	& Standard deviation 	&& Mean  	& Standard deviation 	&& Mean  	& Standard deviation 	&& Mean     & Standard deviation	&& Success ratio \\ \hline
		\multirow{2}{0.05\textwidth}{\centering Overview fly}			& 30 cm/s			& 20 mm					& \gls{drl} 	 && 17.926	&   9.789				&& 3.027 	& 0.638					&& 0.165	& 0.172					&&  30.714  & 	11.407				&& 473/501 \\
																		&					&						& ASL 	 && 22.810	&  17.579				&& 3.477	& 1.958					&& --		& --					&& 324.716  &  173.477				&& 180/501 \\
		\multirow{2}{0.05\textwidth}{\centering Mainly translations}	& 20 cm/s			& 20 mm					& \gls{drl} 	 && 14.162	&   7.990				&& 2.486 	& 0.975					&& 0.983	& 1.874					&&  32.296  & 	10.757				&& 809/926 \\
																		&					&						& ASL 	 && 31.420	&  52.280				&& 2.915	& 0.637					&& --		& --					&& 292.265  &  167.868				&& 358/926 \\
		\multirow{2}{0.05\textwidth}{\centering Mainly rotations}		& 10 cm/s			& 20 mm					& \gls{drl} 	 && 16.576	&   9.389				&& 2.738 	& 0.583					&& 1.214	& 3.148					&&  29.818  &	 9.134				&& 541/968 \\
																		&					&						& ASL 	 && 82.010	& 164.410				&& 3.443	& 4.451					&& --		& --					&& 516.307  &  292.159				&& 185/968 \\
		\hline
	\end{tabu}
	\label{tab:localization-system-evaluation_6-dof-results}
\end{sidewaystable}


\subsection{Point cloud preprocessing}

Point cloud preprocessing can have a very significant role when performing 6 \gls{dof} cloud registration for self-localization. This is due to real-time requirements and also because the computation time increases substantially when registering large point clouds. One way to control this problem is by preprocessing the point cloud with a voxel grid to adjust the level of detail and also assign a limit to the number of points that come from sensors. This can be achieved with random sampling or similar point selection techniques. Moreover, depending on the sensor used, it may be wise to restrict the points to a given range, and discard the rest that are too far way (given that these measurements will have more errors).

In the tests presented in \Cref{tab:localization-system-evaluation_6-dof-results} and performed by the \gls{drl} system it was applied a voxel grid of 20 mm while keeping only the points from the kinect sensor that were at most at 3 meters in the rotations tests and 2.15 meters in the remaining two tests. Moreover it was applied a random sampling filter in order to limit the number of points to 750 in the rotations tests and to 425 in the remaining two tests. In order to allow a fair comparison in terms of processing time, the tests performed with the ethzasl\_icp\_mapper also used a random sample filter with the same limits as the \gls{drl} tests.

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/drl}
%	\caption{Example of filtered point cloud from Kinect data (grid with 0.5 m spacing)}
%	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-drl-filters}
%\end{figure}



\subsection{Point cloud registration}

Looking at \Cref{tab:localization-system-evaluation_6-dof-results} and \crefrange{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-drl-cumulative}{fig:localization-system-evaluation_kinect-translations-robot-movement-path-combined-3d}, it is clear that the \gls{drl} system is able to register point clouds with high accuracy, even when they are severely down-sampled (due to real-time processing constraints). Moreover, the localization system is robust against temporary absence of sensor data (when the field of view of the Kinect was outside the known map in the rotations tests) and was able to quickly recover to accurate tracking when valid sensor data was given (analyzing \crefrange{fig:localization-system-evaluation_kinect-rotations-robot-movement-path-combined-xy}{fig:localization-system-evaluation_kinect-translations-robot-movement-path-combined-3d} it can be seen that the ethzasl\_icp\_mapper was less successful in detecting unreliable pose estimations and recovering from then). In these situations the recovery algorithms were activated, switching the registration algorithm from \gls{icp} point-to-point to \gls{icp} point-to-plane (89 times in 968 registrations in the case of the rotations tests and 0 on the remaining two tests). Given that these sensor data outages were temporary, the initial pose algorithms weren't necessary. Nevertheless, if the Kinect remained outside the map for a longer period of time, the localization system would alert that the tracking was lost and it would try to find its current pose using feature matching.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.33\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/drl-cumulative}
	\caption{Point clouds assembled on top of the map using the DRL poses (for the top part of the figure: green arrows $\rightarrow$ ground truth poses, red arrows $\rightarrow$ DRL poses)}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-drl-cumulative}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.33\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/ethzasl-icp-mapping-cumulative}
	\caption{Point clouds assembled on top of the map using the ethzasl\_icp\_mapper poses (for the top part of the figure: green arrows $\rightarrow$ ground truth poses, red arrows $\rightarrow$ ethzasl\_icp\_mapper poses)}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-ethzasl-icp-mapping-cumulative}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.33\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/ground-truth-cumulative}
	\caption{Point clouds assembled on top of the map using the ground truth poses}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-gt-cumulative}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/robot-movement-path-combined-xy}
	\caption{2D plot XY Poses estimated in the 6 DoF fly test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-fly-robot-movement-path-combined-xy}
\end{figure}

\begin{figure}[H]
	\centering
	\hspace*{0.25cm}\includegraphics[width=0.28\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/robot-movement-path-combined-xz}
	\caption{2D plot XZ Poses estimated in the 6 DoF fly test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-fly-robot-movement-path-combined-xz}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[trim={2.5cm 0 0 1.5cm}, clip, width=0.49\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/robot-movement-path-combined-3d}
	\caption{3D plot XYZ Poses estimated in the 6 DoF fly test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-fly-robot-movement-path-combined-3d}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.355\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/robot-movement-path-combined-xy}
	\caption{2D plot XY Poses estimated in the 6 DoF rotations test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-rotations-robot-movement-path-combined-xy}
\end{figure}

\begin{figure}[H]
	\centering
	\hspace*{0.25cm}\includegraphics[width=0.34\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/robot-movement-path-combined-xz}
	\caption{2D plot XZ Poses estimated in the 6 DoF rotations test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-rotations-robot-movement-path-combined-xz}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[trim={2.5cm 0 0 1.5cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/robot-movement-path-combined-3d}
	\caption{3D plot XYZ Poses estimated in the 6 DoF rotations test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-rotations-robot-movement-path-combined-3d}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.27\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/robot-movement-path-combined-xy}
	\caption{2D plot XY Poses estimated in the 6 DoF translations test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-translations-robot-movement-path-combined-xy}
\end{figure}

\begin{figure}[H]
	\centering
	\hspace*{0.25cm}\includegraphics[width=0.26\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/robot-movement-path-combined-xz}
	\caption{2D plot XZ Poses estimated in the 6 DoF translations test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-translations-robot-movement-path-combined-xz}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[trim={2.5cm 0 0 1.5cm}, clip, width=0.42\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/robot-movement-path-combined-3d}
	\caption{3D plot XYZ Poses estimated in the 6 DoF translations test by the ground truth (green) and DRL (blue) and ethzasl\_icp\_mapper (red)}
	\label{fig:localization-system-evaluation_kinect-translations-robot-movement-path-combined-3d}
\end{figure}

\subsection{Translation and rotation errors}

The results presented in \Cref{tab:localization-system-evaluation_6-dof-results} and in \cref{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-translation-error,fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-rotation-error-asl}, shown that the localization system can maintain high accuracy pose tracking, with translation error below two centimeters and rotation error around 3 degrees when good sensor data is available. Moreover it can quickly recover from temporary registration problems caused by occlusions or accelerations.

Analyzing the results in \Cref{tab:localization-system-evaluation_6-dof-results} and in \crefrange{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-drl-cumulative}{fig:localization-system-evaluation_kinect-translations-robot-movement-path-combined-3d}, it can also be concluded that the \gls{drl} system achieved better pose tracking than the \emph{ethzasl\_icp\_mapper} \gls{ros} package (17.9 mm vs 22.8 mm of translation error and 3.0º vs 3.5º of rotation error), while achieving more than double of the refresh rate in the fly test. For the translations test the \gls{drl} system achieved better tracking in relation to the fly test ()




\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/translation-error-millimeters-distributions}
	\caption{Probability distributions for the DRL translation errors in the 6 DoF fly test}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-translation-error}
\end{figure}

\begin{figure}[H]
	\centering
-	\includegraphics[trim={0 0 0 1cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/rotation-error-degrees-distributions}
	\caption{Probability distributions for the DRL rotation errors in the 6 DoF fly test}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-rotation-error}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/ethzasl-translation-error-millimeters-distributions}
	\caption{Probability distributions for the ethzasl\_icp\_mapper translation errors in the 6 DoF fly test}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-translation-error-asl}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/ethzasl-rotation-error-degrees-distributions}
	\caption{Probability distributions for the ethzasl\_icp\_mapper rotation errors in the 6 DoF fly test}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-rotation-error-asl}
\end{figure}


%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/translation-error-millimeters-distributions}
%	\caption{Probability distributions for the DRL translation errors in the 6 DoF rotations test}
%	\label{fig:localization-system-evaluation_kinect-rotations-10cm-per-sec-velocity-translation-error}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/rotation-error-degrees-distributions}
%	\caption{Probability distributions for the DRL rotation errors in the 6 DoF rotations test}
%	\label{fig:localization-system-evaluation_kinect-rotations-10cm-per-sec-velocity-rotation-error}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/ethzasl-translation-error-millimeters-distributions}
%	\caption{Probability distributions for the ethzasl\_icp\_mapper translation errors in the 6 DoF rotations test}
%	\label{fig:localization-system-evaluation_kinect-rotations-10cm-per-sec-velocity-translation-error-asl}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/ethzasl-rotation-error-degrees-distributions}
%	\caption{Probability distributions for the ethzasl\_icp\_mapper rotation errors in the 6 DoF rotations test}
%	\label{fig:localization-system-evaluation_kinect-rotations-10cm-per-sec-velocity-rotation-error-asl}
%\end{figure}
%
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/translation-error-millimeters-distributions}
%	\caption{Probability distributions for the DRL translation errors in the 6 DoF translations test}
%	\label{fig:localization-system-evaluation_kinect-translations-20cm-per-sec-velocity-translation-error}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/rotation-error-degrees-distributions}
%	\caption{Probability distributions for the DRL rotation errors in the 6 DoF translations test}
%	\label{fig:localization-system-evaluation_kinect-translations-20cm-per-sec-velocity-rotation-error}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/ethzasl-translation-error-millimeters-distributions}
%	\caption{Probability distributions for the ethzasl\_icp\_mapper translation errors in the 6 DoF translations test}
%	\label{fig:localization-system-evaluation_kinect-translations-20cm-per-sec-velocity-translation-error-asl}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/ethzasl-rotation-error-degrees-distributions}
%	\caption{Probability distributions for the ethzasl\_icp\_mapper rotation errors in the 6 DoF translations test}
%	\label{fig:localization-system-evaluation_kinect-translations-20cm-per-sec-velocity-rotation-error-asl}
%\end{figure}



\subsection{Computation time}

The localization system was able to achieve a mean computation time low enough to allow real time processing of the Kinect point cloud sensor data (shown in \Cref{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-computation-time}). Depending on the accuracy requirements, the computation time can be lowered even further by tuning the preprocessing filters, in order to reduce the number of points used in the cloud registration.

Comparing \Cref{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-computation-time} with \Cref{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-computation-time-asl} it can also be seen that the \gls{drl} system required about ten times less processing time in relation to the \emph{ethzasl\_icp\_mapper} system.


\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/computation-times-milliseconds-global-time-distributions}
	\caption{Probability distributions for the DRL global computation time in the 6 DoF translations test}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-computation-time}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[trim={0 0 0 1cm}, clip, width=0.47\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/ethzasl-computation-times-milliseconds-global-time-distributions}
	\caption{Probability distributions for the ethzasl\_icp\_mapper global computation time in the 6 DoF translations test}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-computation-time-asl}
\end{figure}


%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/computation-times-milliseconds-global-time-distributions}
%	\caption{Probability distributions for the DRL global computation time in the 6 DoF rotations test}
%	\label{fig:localization-system-evaluation_kinect-rotations-10cm-per-sec-velocity-computation-time}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-6dof/kinect-rotations-10cm-per-sec-velocity/ethzasl-computation-times-milliseconds-global-time-distributions}
%	\caption{Probability distributions for the ethzasl\_icp\_mapper global computation time in the 6 DoF rotations test}
%	\label{fig:localization-system-evaluation_kinect-rotations-10cm-per-sec-velocity-computation-time-asl}
%\end{figure}
%
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/computation-times-milliseconds-global-time-distributions}
%	\caption{Probability distributions for the DRL global computation time in the 6 DoF translations test}
%	\label{fig:localization-system-evaluation_kinect-translations-20cm-per-sec-velocity-computation-time}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.45\textwidth]{localization-system-evaluation/tests-6dof/kinect-translations-20cm-per-sec-velocity/ethzasl-computation-times-milliseconds-global-time-distributions}
%	\caption{Probability distributions for the ethzasl\_icp\_mapper global computation time in the 6 DoF translations test}
%	\label{fig:localization-system-evaluation_kinect-translations-20cm-per-sec-velocity-computation-time-asl}
%\end{figure}

\subsection{Mapping}

The \gls{drl} system can perform mapping of the environment and is able to build very detailed point clouds. These point clouds can be continuously re-sampled with the Moving Least Squares algorithm (example in \Cref{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-drl-slam}) in order to reconstruct the surfaces of the environment and attenuate the double wall effects and high sensor noise.

Besides the internal mapping capabilities of the \gls{drl} system, it can also be paired with the OctoMap \cite{Hornung2013} library in order to perform probabilistic integration of sensor data and be able to remove missing objects from the reference point cloud.


\begin{figure}[H]
	\centering
	\begin{subfigure}[ht]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-1-gt}
	\end{subfigure}
	\begin{subfigure}[ht]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-2-gt}
	\end{subfigure}
%	\begin{subfigure}[ht]{0.4\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-3-gt}
%	\end{subfigure}
	\begin{subfigure}[ht]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-4-gt}
	\end{subfigure}
	\caption{3D SLAM using the ground truth poses}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-gt-slam}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{subfigure}[ht]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-1}
	\end{subfigure}
	\begin{subfigure}[ht]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-2}
	\end{subfigure}
%	\begin{subfigure}[ht]{0.45\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-3}
%	\end{subfigure}
	\begin{subfigure}[ht]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{localization-system-evaluation/tests-6dof/kinect-fly-30cm-per-sec-velocity/3d-slam-4}
	\end{subfigure}
	\caption{3D SLAM using the localization system with surface reconstruction}
	\label{fig:localization-system-evaluation_kinect-fly-30cm-per-sec-velocity-drl-slam}
\end{figure}
