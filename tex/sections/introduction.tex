\section{Introduction}\label{sec:introduction}

Humanity has sought a reliable method of navigation ever since it started to explore the world. It began with simple landmark reference points for local travels, then perfected celestial navigation for global journeys, and when it finally conquered space, it deployed a global localization system. Autonomous robots face the same problem, because in order to be able to navigate with precision, they first need to know their location.

Over the years, several localization methods have been proposed and refined, according to the navigation environment and the accuracy requirements. Some are meant for high precision local navigation, while others provide an approximate global position.

A robot capable of operating safely and accurately in a dynamic environment can have innumerous applications, ranging from simple delivery tasks to advanced assembly. Besides improving productivity by performing repetitive tasks with precision and speed, robots can also act as coworkers, helping humans perform their jobs more efficiently and thus, reducing the overall production costs.

Mobile robot platforms have a wide range of localization systems to choose from. Odometry is one of the simplest localization methods and relies on proprioceptive information provided by wheel encoders to incrementally update the known pose. But this approach is very sensitive to wheel drift and can accumulate a significant amount of error over time. This method can be improved with filters that model the odometry error, such as Kalman filters \cite{Wan2002}, but most localization systems choose to rely on a combination of proprioceptive and exteroceptive information in order to reliably estimate the robot pose. In \cite{Mautz2012} is presented a very detailed analysis of the available indoor localization systems and the sensing devices that can be used. Most mobile manipulators that require high precision tracking and safety certified sensors rely on \gls{lidar}, since they can operate in a wide range of atmospheric and lighting conditions and provide very accurate measurements of the environment. Nevertheless, stereo vision and RGB-D systems can also achieve very accurate pose estimation and can perform mapping of the environment much faster.

Most of the available localization systems can be categorized as point cloud registration systems, feature registration systems or probabilistic pose estimation systems. Examples of point cloud registration systems such as the ones presented in \cite{Lingemann2005} and \cite{Pomerleau2013} can operate in robots moving at high speeds while the one introduced in \cite{Diosi2005} can directly register clouds in polar coordinates. A different kind of cloud registration is proposed in \cite{Magnusson2009} in which the points normal distributions are used instead of the points themselves. Other systems perform feature matching either from stereo cameras \cite{Kitt2010} or RGB-D sensors \cite{Whelan2013a} and can achieve very high update rate, map the environment really fast and integrate color information besides the geometry itself. Particle filters \cite{Thrun2002} are another group of localization systems that rely on probabilistic models in order to provide robust pose estimation even when the robot becomes temporarily lost.

Some of these localization systems can be used for accurate pose tracking while others provide global pose estimations with less accuracy. The proposed implementation achieves both of these goals and provides an efficient, modular, extensible and easy to configure localization system, capable to operate on a wide range of robot platforms and environments. It is capable of performing high accuracy pose tracking using point cloud registration algorithms and it can also reliably estimate the global position using feature matching. It can use several point cloud sensing devices (such as \glspl{lidar} or RGB-D cameras) and requires no artificial landmarks. Moreover, it can dynamically update the localization map at runtime and can adjust its operation rate based on the estimated sensor velocity in order to use very few hardware resources when the robot platform is not moving. It also offers a detailed analysis of each pose estimation, providing information about the percentage of registered inliers, the root mean square error of the inliers, the angular distribution of the inliers and outliers, the pose corrections that were performed in relation to the previous accepted pose and in case of initial pose estimation it also gives the distribution of the accepted initial poses, which can be very valuable information for a supervisor when the robot is in ambiguous areas that are very similar in different parts of the known map (and as such, requires the navigation supervisor to plot a path to disambiguate the initial pose before beginning any critical operations).

The next section provides a detailed description of the proposed localization system, explaining each main stage of its processing pipeline. \Cref{sec:testing-configurations} describes the testing platforms and environments that were used in the evaluation of the proposed 3/6 \gls{dof} \gls{ros} implementation. \Cref{sec:planar-localization-system-tests,sec:tridimensional-localization-system-tests} discuss the achieved results and \cref{sec:conclusions} finishes with the conclusions.
